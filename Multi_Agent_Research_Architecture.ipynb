{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZXjo0g8aNNA"
      },
      "source": [
        "# **Multi-Agent Research System**\n",
        "This project is a multi-agent system (MAS) built to fulfill the coding task for the RCCIIT Internship shortlisting. The system uses the LangGraph framework  to create a chain of specialized AI agents that work together to answer complex research queries.\n",
        "\n",
        "\n",
        "The architecture is based on the \"Proposed Sample MAS Architecture\" diagram provided in the task.\n",
        "\n",
        "\n",
        "# **üèõÔ∏èProject Architecture**\n",
        "The system is a graph composed of five distinct agents, each with a specific role:\n",
        "\n",
        "### *Query Planner Agent:*\n",
        "This agent receives the user's query. It uses Tree of Thought (ToT) prompting to decompose the query into smaller, atomic questions and determines the best retrieval strategy (internal vector search, external web search, or both).\n",
        "\n",
        "### *Retriever Agent:*\n",
        "This agent conditionally fetches information based on the planner's strategy.\n",
        "\n",
        "It retrieves internal documents from a ChromaDB vector store.\n",
        "\n",
        "It accesses external information using the Tavily web search tool.\n",
        "\n",
        "### *Synthesizer Agent:*\n",
        "This agent takes all the retrieved context (both internal and external) and \"integrates, aligns, filters, and compares\" it to synthesize a comprehensive draft answer.\n",
        "\n",
        "### *Reviewer Agent:*\n",
        "This agent acts as a quality control step. It validates the factual consistency of the synthesized answer against the retrieved context and computes a faithfulness score.\n",
        "\n",
        "### *Writer Agent:*\n",
        "The final agent takes the reviewed answer and formats it into a structured, professional report in Markdown, including citations and the confidence score.\n",
        "\n",
        "# **üöÄTech Stack**\n",
        "This project was built in Google Colab  and utilizes the following technologies:\n",
        "\n",
        "**Graph Framework:** LangGraph\n",
        "\n",
        "\n",
        "**LLM (Open Source):**  Llama 3.1 (via the Groq API)\n",
        "\n",
        "\n",
        "**Vector Database:** ChromaDB\n",
        "\n",
        "**Embedding Model:** HuggingFace \"all-MiniLM-L6-v2\" (runs locally)\n",
        "\n",
        "\n",
        "**Web Search Tool:** Tavily AI\n",
        "\n",
        "Core Libraries: LangChain, Python\n",
        "\n",
        "\n",
        "**Chunking Strategy:** The RAG pipeline demonstrates two chunking strategies (RecursiveCharacterTextSplitter and SentenceTransformersTokenTextSplitter) as required.\n",
        "\n",
        "# **‚öôÔ∏èSetup and Installation**\n",
        "Environment: This notebook is designed to be run in Google Colab.\n",
        "\n",
        "Install Dependencies: The first cell in the notebook installs all required Python packages.\n",
        "\n",
        "#### **Bash--**\n",
        "*[ !pip install -q langgraph langchain langchain_community chromadb tavily-python sentence-transformers langchain-groq langchain-tavily ]*\n",
        "\n",
        "\n",
        "####**API Keys:**\n",
        " When you run the second cell, you will be prompted to enter two API keys:\n",
        "\n",
        "**GROQ_API_KEY:** For the Llama 3.1 model.\n",
        "\n",
        "**TAVILY_API_KEY:** For the web search tool.\n",
        "\n",
        "# **‚ñ∂Ô∏èHow to Run**\n",
        "1. Open the notebook in Google Colab.\n",
        "\n",
        "2. Run all cells from top to bottom, starting with the installation cell.\n",
        "\n",
        "3. The final cell will execute the agent graph.\n",
        "\n",
        "4. You will be prompted to provide your query, like this:\n",
        "\n",
        "**Please enter your research query:** *[Your query here]*\n",
        "\n",
        "**Example Query : Tell me the process of how a Machine learning model can differentiate between different patterns with the help of neural networks ?**\n",
        "\n",
        "The notebook will then print the step-by-step execution of each agent and display the final, formatted report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3V757KQ0AKl",
        "outputId": "ac8a4c87-8758-49a7-e652-7afcb264c67f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Google Colab\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- API keys loaded successfully from .env file ---\n",
            "--- Imports and API key setup complete. ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import TypedDict, List, Optional\n",
        "from dotenv import load_dotenv, find_dotenv  # <-- 1. IMPORT find_dotenv\n",
        "\n",
        "# --- LangChain Core Imports ---\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import AIMessage\n",
        "# ... (all your other imports) ...\n",
        "from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredFileLoader\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_tavily import TavilySearch\n",
        "from langgraph.graph import StateGraph, END\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# --- Load API Keys from .env file ---\n",
        "load_dotenv(find_dotenv())  # <-- 2. USE find_dotenv() HERE\n",
        "\n",
        "# Check if keys are loaded\n",
        "if not os.getenv(\"GROQ_API_KEY\") or not os.getenv(\"TAVILY_API_KEY\"):\n",
        "    print(\"ERROR: API keys not found. Make sure your .env file is in the project root.\")\n",
        "else:\n",
        "    print(\"--- API keys loaded successfully from .env file ---\")\n",
        "\n",
        "print(\"--- Imports and API key setup complete. ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iL1LQB9Kx27"
      },
      "source": [
        "# RAG SETUP & CHUNKING STRATEGIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "rscVwpjoBEXy",
        "outputId": "f9972908-04d1-4323-a80b-48702035e924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- .env file loaded ---\n",
            "--- Initializing LLM and Tools... ---\n",
            "--- LLM and Tools initialized ---\n",
            "--- Setting up RAG... ---\n",
            "Created directory: internal_docs/\n",
            "================================================================================\n",
            "Make sure your files (.pdf, .txt, .doc, .docx) are in the 'internal_docs/' folder.\n",
            "================================================================================\n",
            "Found 3 files. Loading...\n",
            "\n",
            "--- Total documents loaded: 903 ---\n",
            "\n",
            "--- Demonstrating Chunking Strategies... ---\n",
            "Strategy 1 (Recursive) produced 4782 chunks.\n",
            "Strategy 2 (Token-based) produced 6652 chunks.\n",
            "\n",
            "--- Applying Strategy 3 (Agentic Chunking)... ---\n",
            "  - Agentic chunking failed for a document: Could not find JSON object in LLM response: I'm a data scientist with a passion for extracting insights from complex data sets. I have a strong background in machine learning, statistics, and programming, with expertise in languages such as Python, R, and SQL.\n",
            "\n",
            "My day typically starts with reviewing the latest data sets and research papers in my field. I'm always on the lookout for new techniques and methodologies that can help me improve my models and make more accurate predictions.\n",
            "\n",
            "I work closely with cross-functional teams, including business stakeholders, product managers, and engineers, to understand their needs and develop data-driven solutions. Whether it's building predictive models, creating data visualizations, or performing data quality checks, I'm always looking for ways to improve the efficiency and effectiveness of our data-driven decision-making processes.\n",
            "\n",
            "Some of my key skills include:\n",
            "\n",
            "- **Machine learning**: I'm proficient in building and deploying machine learning models using scikit-learn, TensorFlow, and PyTorch.\n",
            "- **Data visualization**: I use libraries like Matplotlib, Seaborn, and Plotly to create interactive and informative visualizations that help stakeholders understand complex data insights.\n",
            "- **Statistical analysis**: I'm well-versed in statistical techniques, including hypothesis testing, regression analysis, and time series analysis.\n",
            "- **Programming**: I'm proficient in Python, R, and SQL, and have experience with other languages like Java and C++.\n",
            "- **Data engineering**: I've worked with big data technologies like Hadoop, Spark, and NoSQL databases to design and implement scalable data pipelines.\n",
            "\n",
            "Some of the tools and technologies I've worked with include:\n",
            "\n",
            "- **Cloud platforms**: AWS, GCP, Azure\n",
            "- **Big data technologies**: Hadoop, Spark, NoSQL databases\n",
            "- **Machine learning frameworks**: scikit-learn, TensorFlow, PyTorch\n",
            "- **Data visualization tools**: Tableau, Power BI, D3.js\n",
            "- **Programming languages**: Python, R, SQL, Java, C++\n",
            "\n",
            "I'm always looking to learn new skills and stay up-to-date with the latest developments in the field. Whether it's attending conferences, reading research papers, or participating in online forums, I'm committed to staying at the forefront of data science and machine learning.\n",
            "\n",
            "What would you like to talk about? Do you have a specific project or problem you'd like to discuss?\n",
            "  - Agentic chunking failed for a document: Could not find JSON object in LLM response: I'm a data scientist with a passion for extracting insights from complex data sets. I have a strong background in machine learning, statistics, and programming, with expertise in languages such as Python, R, and SQL.\n",
            "\n",
            "My day typically starts with reviewing the latest data sets and research papers in my field. I'm always on the lookout for new techniques and methodologies that can help me improve my models and make more accurate predictions.\n",
            "\n",
            "I work closely with cross-functional teams, including business stakeholders, product managers, and engineers, to understand their needs and develop data-driven solutions. Whether it's building predictive models, creating data visualizations, or performing data quality checks, I'm always looking for ways to improve the efficiency and effectiveness of our data-driven decision-making processes.\n",
            "\n",
            "Some of my key skills include:\n",
            "\n",
            "- **Machine learning**: I'm proficient in building and deploying machine learning models using scikit-learn, TensorFlow, and PyTorch.\n",
            "- **Data visualization**: I use libraries like Matplotlib, Seaborn, and Plotly to create interactive and informative visualizations that help stakeholders understand complex data insights.\n",
            "- **Statistical analysis**: I'm well-versed in statistical techniques, including hypothesis testing, regression analysis, and time series analysis.\n",
            "- **Programming**: I'm proficient in Python, R, and SQL, and have experience with other languages like Java and C++.\n",
            "- **Data engineering**: I've worked with big data technologies like Hadoop, Spark, and NoSQL databases to design and implement scalable data pipelines.\n",
            "\n",
            "Some of the tools and technologies I've worked with include:\n",
            "\n",
            "- **Cloud platforms**: AWS, GCP, Azure\n",
            "- **Big data technologies**: Hadoop, Spark, NoSQL databases\n",
            "- **Machine learning frameworks**: scikit-learn, TensorFlow, PyTorch\n",
            "- **Data visualization tools**: Tableau, Power BI, D3.js\n",
            "- **Programming languages**: Python, R, SQL, Java, C++\n",
            "\n",
            "I'm always looking to learn new skills and stay up-to-date with the latest developments in the field. Whether it's attending conferences, reading research papers, or participating in online forums, I'm committed to staying at the forefront of data science and machine learning.\n",
            "\n",
            "What would you like to talk about? Do you have a specific project or problem you'd like to discuss?\n",
            "  - Agentic chunking failed for a document: Could not find JSON object in LLM response: I'm a data scientist with a passion for extracting insights from complex data sets. I have a strong background in machine learning, statistics, and programming, with expertise in languages such as Python, R, and SQL.\n",
            "\n",
            "My day typically starts with reviewing the latest data sets and research papers in my field. I'm always on the lookout for new techniques and methodologies that can help me improve my models and make more accurate predictions.\n",
            "\n",
            "I work closely with cross-functional teams, including business stakeholders, product managers, and engineers, to understand their needs and develop data-driven solutions. Whether it's building predictive models, creating data visualizations, or performing data quality checks, I'm always looking for ways to improve the efficiency and effectiveness of our data-driven decision-making processes.\n",
            "\n",
            "Some of my key skills include:\n",
            "\n",
            "- **Machine learning**: I'm proficient in building and deploying machine learning models using scikit-learn, TensorFlow, and PyTorch.\n",
            "- **Data visualization**: I use libraries like Matplotlib, Seaborn, and Plotly to create interactive and informative visualizations that help stakeholders understand complex data insights.\n",
            "- **Statistical analysis**: I'm well-versed in statistical techniques, including hypothesis testing, regression analysis, and time series analysis.\n",
            "- **Programming**: I'm proficient in Python, R, and SQL, and have experience with other languages like Java and C++.\n",
            "- **Data engineering**: I've worked with big data technologies like Hadoop, Spark, and NoSQL databases to design and implement scalable data pipelines.\n",
            "\n",
            "Some of the tools and technologies I've worked with include:\n",
            "\n",
            "- **Cloud platforms**: AWS, GCP, Azure\n",
            "- **Big data technologies**: Hadoop, Spark, NoSQL databases\n",
            "- **Machine learning frameworks**: scikit-learn, TensorFlow, PyTorch\n",
            "- **Data visualization tools**: Tableau, Power BI, D3.js\n",
            "- **Programming languages**: Python, R, SQL, Java, C++\n",
            "\n",
            "I'm always looking to learn new skills and stay up-to-date with the latest developments in the field. Whether it's attending conferences, reading research papers, or participating in online forums, I'm committed to staying at the forefront of data science and machine learning.\n",
            "\n",
            "What would you like to talk about? Do you have a specific project or problem you'd like to discuss?\n",
            "Strategy 3 (Agentic) produced 3 chunks.\n",
            "\n",
            "--- Initializing HuggingFace Embeddings... ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dibya\\AppData\\Local\\Temp\\ipykernel_25844\\989768449.py:123: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Vector Store Created Successfully. ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from dotenv import load_dotenv  # <-- Now this import will work\n",
        "from typing import TypedDict, List, Optional\n",
        "\n",
        "# --- LangChain Core Imports ---\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "# --- LangChain Community Imports ---\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader, UnstructuredFileLoader\n",
        "\n",
        "# --- LangChain Partner Package Imports ---\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "# --- LangGraph Imports ---\n",
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "# --- Standard Library Imports ---\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# --- 1. Load API Keys from .env file ---\n",
        "load_dotenv() \n",
        "print(\"--- .env file loaded ---\")\n",
        "\n",
        "# --- 2. Initialize Models and Tools (FIXED) ---\n",
        "print(\"--- Initializing LLM and Tools... ---\")\n",
        "\n",
        "# Manually get keys from environment\n",
        "groq_key = os.getenv(\"GROQ_API_KEY\")\n",
        "tavily_key = os.getenv(\"TAVILY_API_KEY\")\n",
        "\n",
        "# Check if keys were actually loaded\n",
        "if not groq_key:\n",
        "    raise ValueError(\"GROQ_API_KEY not found. Check your .env file.\")\n",
        "if not tavily_key:\n",
        "    raise ValueError(\"TAVILY_API_KEY not found. Check your .env file.\")\n",
        "\n",
        "# Pass keys directly as parameters to fix API key errors\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0, groq_api_key=groq_key)\n",
        "web_search_tool = TavilySearch(k=3, tavily_api_key=tavily_key)\n",
        "\n",
        "print(\"--- LLM and Tools initialized ---\")\n",
        "\n",
        "# --- 3. Define Helper Function ---\n",
        "def extract_json_from_response(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the JSON blob from an LLM response.\n",
        "    \"\"\"\n",
        "    start = text.find('{')\n",
        "    end = text.rfind('}')\n",
        "    if start == -1 or end == -1:\n",
        "        raise ValueError(f\"Could not find JSON object in LLM response: {text}\")\n",
        "    return text[start:end+1]\n",
        "\n",
        "# --- 4. RAG Setup ---\n",
        "print(\"--- Setting up RAG... ---\")\n",
        "doc_dir = \"internal_docs/\" # Use a relative path for VS Code\n",
        "os.makedirs(doc_dir, exist_ok=True)\n",
        "print(f\"Created directory: {doc_dir}\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Make sure your files (.pdf, .txt, .doc, .docx) are in the '{doc_dir}' folder.\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "all_documents = []\n",
        "loaded_files = os.listdir(doc_dir)\n",
        "\n",
        "if not loaded_files:\n",
        "    print(\"No files found. Please add files to 'internal_docs' and re-run.\")\n",
        "    retriever = None\n",
        "else:\n",
        "    print(f\"Found {len(loaded_files)} files. Loading...\")\n",
        "    for file_name in loaded_files:\n",
        "        file_path = os.path.join(doc_dir, file_name)\n",
        "        try:\n",
        "            if file_name.endswith(\".pdf\"):\n",
        "                loader = PyMuPDFLoader(file_path)\n",
        "                all_documents.extend(loader.load())\n",
        "            elif file_name.endswith((\".txt\", \".doc\", \".docx\")):\n",
        "                loader = UnstructuredFileLoader(file_path)\n",
        "                all_documents.extend(loader.load())\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {file_name}: {e}\")\n",
        "\n",
        "    print(f\"\\n--- Total documents loaded: {len(all_documents)} ---\")\n",
        "    \n",
        "    # --- 5. Chunking Strategies ---\n",
        "    print(\"\\n--- Demonstrating Chunking Strategies... ---\")\n",
        "    recursive_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    chunks_recursive = recursive_splitter.split_documents(all_documents)\n",
        "    print(f\"Strategy 1 (Recursive) produced {len(chunks_recursive)} chunks.\")\n",
        "\n",
        "    token_splitter = SentenceTransformersTokenTextSplitter(chunk_overlap=20, tokens_per_chunk=100)\n",
        "    chunks_token = token_splitter.split_documents(all_documents)\n",
        "    print(f\"Strategy 2 (Token-based) produced {len(chunks_token)} chunks.\")\n",
        "\n",
        "    # --- Strategy 3: Agentic Chunking ---\n",
        "    print(\"\\n--- Applying Strategy 3 (Agentic Chunking)... ---\")\n",
        "    def agentic_chunker(document, llm_model):\n",
        "        prompt = f\"\"\"You are a data scientist... (Your full chunking prompt) ...\"\"\"\n",
        "        try:\n",
        "            response = llm_model.invoke(prompt)\n",
        "            json_string = extract_json_from_response(response.content)\n",
        "            data = json.loads(json_string)\n",
        "            agent_chunks = [Document(page_content=chunk[\"content\"], metadata={\"source\": document.metadata.get(\"source\", \"unknown\"), \"agentic_topic\": chunk[\"topic_name\"]}) for chunk in data.get(\"chunks\", [])]\n",
        "            return agent_chunks\n",
        "        except Exception as e:\n",
        "            print(f\"  - Agentic chunking failed for a document: {e}\")\n",
        "            return [document]\n",
        "\n",
        "    chunks_agentic = []\n",
        "    # Process only the first 3 docs to save time\n",
        "    for doc in all_documents[:3]: \n",
        "        chunks_agentic.extend(agentic_chunker(doc, llm)) \n",
        "    print(f\"Strategy 3 (Agentic) produced {len(chunks_agentic)} chunks.\")\n",
        "\n",
        "    # --- 6. Create Vector Store ---\n",
        "    print(\"\\n--- Initializing HuggingFace Embeddings... ---\")\n",
        "    embedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "    vectorstore = Chroma.from_documents(documents=chunks_recursive, embedding=embedding_function)\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    print(\"--- Vector Store Created Successfully. ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw8YxksgLON-"
      },
      "source": [
        "# DEFINE AGENTS AND TOOLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJvmNVM0GUDn",
        "outputId": "444f5772-c31d-43f2-8622-d5320b688b38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Defining Agents and Tools... ---\n",
            "--- All agents defined successfully. ---\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_tavily import TavilySearch\n",
        "\n",
        "print(\"--- Defining Agents and Tools... ---\")\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
        "web_search_tool = TavilySearch(k=3)\n",
        "\n",
        "\n",
        "def extract_json_from_response(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the JSON blob from an LLM response that might include\n",
        "    markdown backticks.\n",
        "    \"\"\"\n",
        "    start = text.find('{')\n",
        "    end = text.rfind('}')\n",
        "\n",
        "    if start == -1 or end == -1:\n",
        "        raise ValueError(f\"Could not find JSON object in LLM response: {text}\")\n",
        "\n",
        "    return text[start:end+1]\n",
        "\n",
        "# --- 1. Query Planner Agent ---\n",
        "def query_planner_agent(state):\n",
        "    \"\"\"\n",
        "    Decomposes the user query and determines the retrieval strategy\n",
        "    using Tree of Thought (ToT) prompting.\n",
        "    \"\"\"\n",
        "    print(\"--- üß† Executing Query Planner ---\")\n",
        "\n",
        "    prompt = f\"\"\"You are a meticulous and strategic research planner. Your role is to analyze a complex user query and break it down into a logical, step-by-step execution plan for a team of agents.\n",
        "\n",
        "    **User Query:** \"{state['original_query']}\"\n",
        "\n",
        "    **Your Task:**\n",
        "    1.  **Decompose:** Analyze the query and break it down into smaller, self-contained, atomic questions that must be answered to fulfill the user's request.\n",
        "    2.  **Strategize:** For each atomic question, decide the best retrieval source. Use 'internal_vector_search' for questions about proprietary data, internal documents, or specific in-house knowledge. Use 'web_search' for general knowledge, definitions, market comparisons, or public information. If a query requires both, use 'both'.\n",
        "    3.  **Synthesize:** Consolidate this plan into a single, valid JSON object.\n",
        "\n",
        "    **Output Format:**\n",
        "    You MUST provide only the final JSON object as your response. Do not add any text, markdown, or commentary before or after the JSON block.\n",
        "    The JSON object must have two keys:\n",
        "    1.  `decomposed_queries`: A list of **strings**, where each string is an atomic query.\n",
        "    2.  `retrieval_strategy`: A single string: 'internal_vector_search', 'web_search', or 'both'.\n",
        "\n",
        "    **Example:**\n",
        "    `{{\"decomposed_queries\": [\"What is the in-house DL framework?\", \"Who are the top 3 market leaders in deep learning?\", \"Benchmark performance of in-house vs market leaders\"], \"retrieval_strategy\": \"both\"}}`\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    try:\n",
        "        json_string = extract_json_from_response(response.content)\n",
        "        plan = json.loads(json_string)\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing JSON from Planner: {e}\")\n",
        "        print(f\"LLM Response was: {response.content}\")\n",
        "        raise\n",
        "\n",
        "    return {\"decomposed_queries\": plan['decomposed_queries'], \"retrieval_strategy\": plan['retrieval_strategy']}\n",
        "\n",
        "# --- 2. Retriever Agent ---\n",
        "def retriever_agent(state):\n",
        "    \"\"\"\n",
        "    Retrieves information based on the planner's strategy.\n",
        "    \"\"\"\n",
        "    print(f\"--- üîç Executing Retriever (Strategy: {state['retrieval_strategy']}) ---\")\n",
        "    context = []\n",
        "\n",
        "    query_list = state['decomposed_queries']\n",
        "\n",
        "    for q in query_list:\n",
        "        query_string = \"\"\n",
        "        if isinstance(q, dict):\n",
        "            if 'query' in q:\n",
        "                query_string = q['query']\n",
        "            elif 'question' in q:\n",
        "                query_string = q['question']\n",
        "            elif q.values():\n",
        "                query_string = str(list(q.values())[0])\n",
        "        elif isinstance(q, str):\n",
        "            query_string = q\n",
        "\n",
        "        if not query_string:\n",
        "            continue\n",
        "\n",
        "        if state['retrieval_strategy'] in ['internal_vector_search', 'both']:\n",
        "            if retriever: # Check if retriever was successfully created\n",
        "                docs = retriever.invoke(query_string)\n",
        "                context.extend([f\"Internal Doc: {d.page_content}\" for d in docs])\n",
        "            else:\n",
        "                print(\"Warning: Internal search requested, but retriever is not available.\")\n",
        "\n",
        "        if state['retrieval_strategy'] in ['web_search', 'both']:\n",
        "            search_results = web_search_tool.invoke(query_string)\n",
        "            context.extend([f\"Web Result: {res}\" for res in search_results])\n",
        "\n",
        "    return {\"retrieved_context\": context}\n",
        "\n",
        "# --- 3. Synthesizer Agent ---\n",
        "def synthesizer_agent(state):\n",
        "    \"\"\"\n",
        "    Integrates and synthesizes the retrieved context into a draft answer.\n",
        "    \"\"\"\n",
        "    print(\"--- ‚úçÔ∏è Executing Synthesizer ---\")\n",
        "    prompt = f\"\"\"You are a critical research analyst and synthesizer. Your goal is not to just list information, but to **weave a comprehensive, neutral, and coherent narrative** from the provided context.\n",
        "\n",
        "    **Original Query:** \"{state['original_query']}\"\n",
        "    \n",
        "    **Retrieved Context (from internal docs and web search):**\n",
        "    ---\n",
        "    {state['retrieved_context']}\n",
        "    ---\n",
        "\n",
        "    **Your Task:**\n",
        "    1.  Read the Original Query to understand the user's core intent.\n",
        "    2.  Thoroughly read all pieces of Retrieved Context.\n",
        "    3.  **Critically analyze** the context. If you find both 'Internal Doc' and 'Web Result' sources, you **must** compare them.\n",
        "    4.  Write a single, flowing draft answer that directly addresses the Original Query.\n",
        "    5.  **Important:** When information conflicts (e.g., an internal doc says one thing, a web result says another), you must **explicitly point out this discrepancy** in your draft.\n",
        "    6.  **Do not add any information or make any assumptions** that are not present in the Retrieved Context. Your answer must be 100% grounded in the provided sources.\n",
        "\n",
        "    **Draft Answer:**\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return {\"synthesized_answer\": response.content}\n",
        "\n",
        "# --- 4. Reviewer Agent ---\n",
        "def reviewer_agent(state):\n",
        "    \"\"\"\n",
        "    Validates the synthesized answer for factual consistency and\n",
        "    computes a faithfulness score.\n",
        "    \"\"\"\n",
        "    print(\"--- ‚úÖ Executing Reviewer ---\")\n",
        "    prompt = f\"\"\"You are a meticulous fact-checking and quality assurance editor. Your sole purpose is to ensure the `synthesized_answer` is 100% faithful to the `retrieved_context` and fully complete.\n",
        "\n",
        "    **Original Query:** \"{state['original_query']}\"\n",
        "\n",
        "    **Retrieved Context:**\n",
        "    ---\n",
        "    {state['retrieved_context']}\n",
        "    ---\n",
        "    \n",
        "    **Synthesized Answer (Draft):**\n",
        "    ---\n",
        "    {state['synthesized_answer']}\n",
        "    ---\n",
        "\n",
        "    **Your Task:**\n",
        "    1.  **Faithfulness Check:** Read the `synthesized_answer` sentence by sentence. Verify that *every single claim* is directly supported by a fact in the `retrieved_context`.\n",
        "    2.  **Completeness Check:** Ensure the draft answer addresses all parts of the `original_query`.\n",
        "    3.  **Provide a Score:** Give a `faithfulness_score` from 0.0 (total fabrication) to 1.0 (perfectly supported).\n",
        "    4.  **Provide Notes:** Briefly explain your score. If the score is less than 1.0, you **must** flag the specific claims that were unfaithful or fabricated.\n",
        "\n",
        "    **Output Format:**\n",
        "    You MUST return a single, valid JSON object. Do not add any text before or after it.\n",
        "    The JSON object **must** have a key named `faithfulness_score` (a float) and a key named `review_notes` (a string).\n",
        "    \n",
        "    **Example:**\n",
        "    `{{\"faithfulness_score\": 0.8, \"review_notes\": \"The answer is mostly faithful, but the claim about 'v3.0' is not supported by the context.\"}}`\n",
        "    \"\"\"\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    try:\n",
        "        json_string = extract_json_from_response(response.content)\n",
        "        review = json.loads(json_string)\n",
        "\n",
        "        print(f\"Reviewer LLM Response (Parsed): {review}\")\n",
        "\n",
        "        score = 0.0 # Default\n",
        "        if 'faithfulness_score' in review:\n",
        "            score = review['faithfulness_score']\n",
        "        elif 'score' in review:\n",
        "            score = review['score']\n",
        "        elif 'faithfulness' in review:\n",
        "            score = review['faithfulness']\n",
        "        else:\n",
        "            print(\"Warning: Could not find 'faithfulness_score' or 'score' in reviewer output. Defaulting to 0.0\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing JSON from Reviewer: {e}\")\n",
        "        print(f\"LLM Response was: {response.content}\")\n",
        "        raise\n",
        "\n",
        "    return {\"reviewed_answer\": state['synthesized_answer'], \"faithfulness_score\": score}\n",
        "\n",
        "# --- 5. Writer Agent (IMPROVED MARKDOWN) ---\n",
        "def writer_agent(state):\n",
        "    \"\"\"\n",
        "    Generates the final, structured report with citations and confidence.\n",
        "    \"\"\"\n",
        "    print(\"--- üìÑ Executing Writer ---\")\n",
        "    prompt = f\"\"\"You are a **senior technical writer** preparing a final report for an executive briefing. Your writing style is **formal, objective, clear, and concise**.\n",
        "\n",
        "    **Reviewed Answer:** \"{state['reviewed_answer']}\"\n",
        "    **Faithfulness Score:** {state['faithfulness_score']}\n",
        "\n",
        "    **Your Task:**\n",
        "    Transform the `reviewed_answer` into a polished, professional report.\n",
        "    \n",
        "    The report **must** use the following Markdown structure:\n",
        "    \n",
        "    # [Create a Clear, Descriptive Title for the Report]\n",
        "    \n",
        "    ##  Executive Summary\n",
        "    (Write a 1-2 sentence summary of the main answer here.)\n",
        "    \n",
        "    ---\n",
        "    \n",
        "    ## Detailed Findings\n",
        "    (Format the main body of the `reviewed_answer` here. Use `###` H3 headings for sub-topics and bullet points for lists to make it scannable and easy to read.)\n",
        "    \n",
        "    ---\n",
        "    \n",
        "    ## Confidence & Sources\n",
        "    \n",
        "    ### Confidence Score\n",
        "    **{state['faithfulness_score']*100:.0f}%**\n",
        "    \n",
        "    ### Sources\n",
        "    (Cite the information from the 'Internal Doc' and 'Web Result' bold labels found in the reviewed answer. Format as a bulleted list.)\n",
        "    ### References\n",
        "    (Cite all references used in the report here in a bulleted list.)\n",
        "    **Final Report (in Markdown format):**\n",
        "    \"\"\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return {\"final_report\": response.content}\n",
        "\n",
        "print(\"--- All agents defined successfully. ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXunsfFbXRn8"
      },
      "source": [
        "# ASSEMBLE THE LANGGRAPH WORKFLOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9AGO4EhM3pC",
        "outputId": "0b7739f6-9741-49b7-c497-b3cfa7c997a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Assembling LangGraph workflow... ---\n",
            "--- Workflow compiled successfully. Ready to run. ---\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Assembling LangGraph workflow... ---\")\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    original_query: str\n",
        "    decomposed_queries: List[str]\n",
        "    retrieval_strategy: str\n",
        "    retrieved_context: List[str]\n",
        "    synthesized_answer: str\n",
        "    reviewed_answer: str\n",
        "    faithfulness_score: Optional[float]\n",
        "    final_report: str\n",
        "\n",
        "workflow = StateGraph(GraphState)\n",
        "\n",
        "workflow.add_node(\"planner\", query_planner_agent)\n",
        "workflow.add_node(\"retriever\", retriever_agent)\n",
        "workflow.add_node(\"synthesizer\", synthesizer_agent)\n",
        "workflow.add_node(\"reviewer\", reviewer_agent)\n",
        "workflow.add_node(\"writer\", writer_agent)\n",
        "\n",
        "workflow.set_entry_point(\"planner\")\n",
        "workflow.add_edge(\"planner\", \"retriever\")\n",
        "workflow.add_edge(\"retriever\", \"synthesizer\")\n",
        "workflow.add_edge(\"synthesizer\", \"reviewer\")\n",
        "workflow.add_edge(\"reviewer\", \"writer\")\n",
        "workflow.add_edge(\"writer\", END)\n",
        "\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"--- Workflow compiled successfully. Ready to run. ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-gWG5dHXK0L"
      },
      "source": [
        "# EXECUTION AND FINAL OUTPUT\n",
        "## This cell runs the compiled graph with a user-provided query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "4HBXMCn4M8ZS",
        "outputId": "d85ffab7-fd77-4825-fbd3-d80e53a75723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Executing the workflow... ---\n",
            "User Input : How CNN is used for image processing ?\n",
            "--- üß† Executing Query Planner ---\n",
            "--- üîç Executing Retriever (Strategy: both) ---\n",
            "--- ‚úçÔ∏è Executing Synthesizer ---\n",
            "--- ‚úÖ Executing Reviewer ---\n",
            "Reviewer LLM Response (Parsed): {'faithfulness_score': 0.95, 'review_notes': \"The answer is mostly faithful, but the claim about 'standard benchmarks for deep learning algorithms in computer vision include object recognition and OCR' is not explicitly mentioned in the context. However, it can be inferred from the context that object recognition is a task that CNNs are applied to.\"}\n",
            "--- üìÑ Executing Writer ---\n",
            "\n",
            "--- ‚úÖ GRAPH EXECUTION COMPLETE ---\n",
            "Final Report:\n"
          ]
        },
        {
          "data": {
            "text/markdown": [
              "# Understanding CNNs in Image Processing\n",
              "\n",
              "## Executive Summary\n",
              "This report provides an overview of Convolutional Neural Networks (CNNs) and their application in image processing. It highlights the key mechanisms behind CNNs and their importance in computer vision tasks.\n",
              "\n",
              "---\n",
              "\n",
              "## Detailed Findings\n",
              "\n",
              "### Convolutional Neural Networks (CNNs)\n",
              "\n",
              "CNNs are a type of neural network that is widely applied to image data. They are designed to be invariant to certain transformations of the inputs, such as translation, rotation, and scaling. This is achieved through three mechanisms:\n",
              "\n",
              "* Local receptive fields: Units in a feature map take inputs only from a small subregion of the image.\n",
              "* Weight sharing: The same feature is computed across different regions of the image.\n",
              "* Subsampling: Units in a feature map detect the same pattern but at different locations in the input image.\n",
              "\n",
              "According to internal documentation, the structure of a convolutional network is illustrated in Figure 5.17, which shows a layer of convolutional units followed by a layer of subsampling units. Several successive pairs of such layers may be used.\n",
              "\n",
              "### Parameter Sharing\n",
              "\n",
              "Internal documentation highlights the concept of parameter sharing, where the same feature is computed across different regions of the image. This leads to large memory savings, as we need to store only a subset of the parameters. For example, in Convolutional Neural Networks or CNNs, the same feature is computed across different regions of the image, and hence, a cat is detected irrespective of whether it is at the top or bottom of the image.\n",
              "\n",
              "### Computer Vision\n",
              "\n",
              "Computer vision is one of the most active areas for deep learning research, as it is a task effortless for humans but difficult for computers. Standard benchmarks for deep learning algorithms in computer vision include object recognition and OCR (Optical Character Recognition). Computer vision requires little preprocessing, and images should be standardized, so pixels lie in the same range.\n",
              "\n",
              "### Comparison of Sources\n",
              "\n",
              "Upon comparing the internal documentation and web results, we notice that both sources agree on the importance of CNNs in image processing. However, there is a discrepancy in the level of detail provided. The internal documentation provides a more in-depth explanation of the mechanisms behind CNNs, while the web results provide a more general overview of the topic.\n",
              "\n",
              "---\n",
              "\n",
              "## Confidence & Sources\n",
              "\n",
              "### Confidence Score\n",
              "**95%**\n",
              "\n",
              "### Sources\n",
              "* Internal documentation: Provided a detailed explanation of the mechanisms behind CNNs, including local receptive fields, weight sharing, and subsampling.\n",
              "* Web results: Provided a general overview of the importance of CNNs in image processing.\n",
              "\n",
              "### References\n",
              "* Internal documentation: Not publicly available.\n",
              "* Web results: Various online sources, including [1] and [2].\n",
              "\n",
              "[1] - [Insert reference 1]\n",
              "[2] - [Insert reference 2]\n",
              "\n",
              "Note: The references section should include all sources used in the report, including internal documentation and web results. However, since the internal documentation is not publicly available, it is not included in the references section."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(\"--- Executing the workflow... ---\")\n",
        "\n",
        "query = input(\"Please enter your research query: \")\n",
        "# Example query : \n",
        "# 1.Tell me the process of how a Machine learning model can differentiate between different patterns with the help of neural networks ?\n",
        "# 2.How CNN is used for image processing ?\n",
        "print(\"User Input :\",query)\n",
        "\n",
        "initial_state = {\"original_query\": query}\n",
        "\n",
        "final_state = app.invoke(initial_state)\n",
        "\n",
        "print(\"\\n--- ‚úÖ GRAPH EXECUTION COMPLETE ---\")\n",
        "print(\"Final Report:\")\n",
        "display(Markdown(final_state['final_report']))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.10"
      }
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
